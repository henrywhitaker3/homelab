apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: misc
spec:
  groups:
    - name: playground
      rules:
        - alert: APILatencyAnomaly
          expr: |
            (
              (
                avg_over_time(srep_request_latency_seconds[1m])
                  -
                avg_over_time(srep_request_latency_seconds[1d])
              )
                /
              stddev_over_time(srep_request_latency_seconds[1d])
            ) > 3
          for: 5m
          labels:
            severity: warning
            experimental: "true"
          annotations:
            summary: Request latency anomaly for {{ $labels.url }}
    - name: pve
      rules:
        - alert: PVEDiskFillingUp
          expr: (sum by (id) (pve_disk_usage_bytes{id=~".*local-lvm"}) / sum by (id) (pve_disk_size_bytes{id=~".*local-lvm"})) * 100 > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: PVE local-lvm disk filling on {{ $labels.id }}
    - name: loki
      rules:
        - alert: LokiMetricsWriteFailed
          expr: increase (loki_ruler_wal_prometheus_remote_storage_samples_failed_total[1m]) > 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Loki failed to write metrics to prometheus
    - name: longhorn
      rules:
        - alert: BackupFailed
          expr: count by (backup, volume) (longhorn_backup_state > 3) > 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Backup {{ $labels.backup }} failed for {{ $labels.volume }}
        - alert: VolumeFillingUp
          expr: ((avg by (pvc) (longhorn_volume_actual_size_bytes))/ (avg by (pvc) (longhorn_volume_capacity_bytes))) * 100 > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Volume {{ $labels.pvc }} is using > 90% of capactiy
        - alert: VolumeStatusCritical
          expr: longhorn_volume_robustness == 3
          for: 5m
          annotations:
            summary: Longhorn volume {{$labels.volume}} is Fault
          labels:
            severity: critical
        - alert: VolumeStatusWarning
          expr: longhorn_volume_robustness == 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Longhorn volume {{$labels.volume}} is Degraded
        - alert: NodeNotSchedulable
          expr: longhorn_node_status{condition="schedulable"} != 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Longhorn Node {{ $labels.node }} unschedulable due to {{ $labels.condition_reason }}
    - name: vms
      rules:
        - alert: VMDown
          expr: 100 * (count by (instance) (up{job="node_exporter"} == 0) / count by (instance) (up{job="node_exporter"})) > 10
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Target {{ $labels.instance }} is DOWN
        - alert: CPULoad5Anomaly
          expr: (anomaly:node_load5_1w > 4) and on(instance) (node_load5 > 2)
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: CPU Load 5 on {{ $labels.instance }} is > 3 standard deviations from expected for 5 mins
        - alert: CPUIdleAnomaly
          expr: abs(anomaly:node_cpu_seconds_idle_1w) > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: CPU Idle Time on {{ $labels.instance }} is > 3 standard deviations from expected for 5 mins
        - alert: NetworkTxAnomaly
          expr: node_network_receive_bytes_1w > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Network Transmits on {{ $labels.instance }} is > 3 standard deviations from expected for 5 mins
        - alert: NetworkRxAnomaly
          expr: node_network_receive_bytes_1w > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Network Received on {{ $labels.instance }} is > 3 standard deviations from expected for 5 mins
    - name: k8s
      rules:
        - alert: OOMKilled
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
          labels:
            severity: critical
          annotations:
            summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
    - name: haproxy
      rules:
        - alert: HaproxyServerNotUp
          expr: haproxy_server_status{state!="UP"} > 0
          for: 3m
          labels:
            severity: critical
          annotations:
            summary: Haproxy Server {{ $labels.server }} is not UP for backend {{ $labels.proxy }}
    - name: argo
      rules:
        - alert: ArgocdServiceUnhealthy
          expr: argocd_app_info{health_status!="Healthy"} != 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: ArgoCD service unhealthy (instance {{ $labels.name }})
        - alert: ArgocdServiceNotSynced
          expr: argocd_app_info{sync_status!="Synced"} != 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: ArgoCD service not synced (instance {{ $labels.name }})
    - name: db
      rules:
        - alert: GaleraClusterSize
          expr: |
            (
              min(mysql_global_status_wsrep_cluster_size{job="mariadb-metrics", namespace="databases"})
                -
              sum(kube_statefulset_replicas{namespace="databases", statefulset="mariadb"})
            ) != 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Galera cluster size is not equal to the number of replicase
        - alert: GaleraBackupFailed
          expr: |
            sum(kube_job_status_failed{namespace="databases", job_name=~"mariabackup.*"}) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Galera backup job failed
        - alert: MysqlDown
          expr: mysql_up == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: MySQL down (instance {{ $labels.instance }})
        - alert: MysqlHighThreadsRunning
          expr: max_over_time(mysql_global_status_threads_running[1m]) / mysql_global_variables_max_connections * 100 > 75
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: MySQL high threads running (instance {{ $labels.instance }})
        - alert: RedisDown
          expr: redis_up == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Redis down (instance {{ $labels.instance }})
