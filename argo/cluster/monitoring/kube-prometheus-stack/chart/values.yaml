namespaceOverride: monitoring

alertmanager:
  enabled: true

  config:
    route:
      routes:
        - receiver: discord
          match:
            severity: critical
    receivers:
      - name: "null"
      - name: discord
        # discord_configs:
        #   - webhook_url_file: /etc/alertmanager/secrets/alertmanager-secrets/discord

  alertmanagerSpec:
    replicas: 2
    secrets:
      - alertmanager-secrets

  ingress:
    enabled: true

    annotations:
      kubernetes.io/tls-acme: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-url: https://auth.plexmox.com/api/verify
      nginx.ingress.kubernetes.io/auth-signin: https://auth.plexmox.com

    hosts:
      - alerts.plexmox.com

    tls:
      - secretName: alerts-plexmox-com-tls
        hosts:
          - alerts.plexmox.com

nodeExporter:
  enabled: false

kubeEtcd:
  enabled: true
  endpoints:
    - 10.0.0.20
    - 10.0.0.21
    - 10.0.0.24

kubeControllerMnager:
  enabled: true
  endpoints:
    - 10.0.0.20
    - 10.0.0.21
    - 10.0.0.24

kubeProxy:
  enabled: true
  endpoints:
    - 10.0.0.20
    - 10.0.0.21
    - 10.0.0.24

kubeScheduler:
  enabled: true
  endpoints:
    - 10.0.0.20
    - 10.0.0.21
    - 10.0.0.24

defaultRules:
  rules:
    kubeControllerManager: false
    kubeProxy: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false

additionalPrometheusRulesMap:
  misc:
    groups:
      - name: vms
        rules:
          - alert: VMDown
            expr: 100 * (count by (instance) (up{job="node_exporter"} == 0) / count by (instance) (up{job="node_exporter"})) > 10
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: Target {{ $labels.instance }} is DOWN
      - name: k8s
        rules:
          - alert: OOMKilled
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            labels:
              severity: critical
            annotations:
              summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
      - name: haproxy
        rules:
          - alert: HaproxyServerNotUp
            expr: haproxy_server_status{state!="UP"} > 0
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: Haproxy Server {{ $labels.server }} is not UP for backend {{ $labels.proxy }}
      - name: argo
        rules:
          - alert: ArgocdServiceUnhealthy
            expr: argocd_app_info{health_status!="Healthy"} != 0
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: ArgoCD service unhealthy (instance {{ $labels.instance }})
          - alert: ArgocdServiceNotSynced
            expr: argocd_app_info{sync_status!="Synced"} != 0
            for: 15m
            labels:
              severity: warning
            annotations:
              summary: ArgoCD service not synced (instance {{ $labels.instance }})
      - name: db
        rules:
          - alert: MysqlDown
            expr: mysql_up{service!~"srep.*"} == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: MySQL down (instance {{ $labels.instance }})
          - alert: MysqlHighThreadsRunning
            expr: max_over_time(mysql_global_status_threads_running{service!~"srep.*"}[1m]) / mysql_global_variables_max_connections{service!~"srep.*"} * 100 > 75
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: MySQL high threads running (instance {{ $labels.instance }})
          - alert: RedisDown
            expr: redis_up{service!~"srep*"} == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis down (instance {{ $labels.instance }})
  srep:
    groups:
      - name: api
        rules:
          - alert: APIErrorRate
            expr: ((rate(srep_requests_total{code=~"2..|4..|3.."}[1m]) / rate(srep_requests_total[1m])) * 100) < 99.99
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: API Success Rate is < 99.99%
          - alert: APIMysqlDown
            expr: mysql_up{service="srep-mysql"} == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: API MySQL is down
          - alert: APIMysqlHighThreadsRunning
            expr: max_over_time(mysql_global_status_threads_running{service="srep-mysql"}[1m]) / mysql_global_variables_max_connections{service="srep-mysql"} * 100 > 75
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: MySQL high threads running (instance {{ $labels.instance }})
          - alert: APIRedisDown
            expr: redis_up{service="srep-redis"} == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis down (instance {{ $labels.instance }})

grafana:
  enabled: true

  admin:
    existingSecret: grafana-creds

  ingress:
    enabled: true

    annotations:
      kubernetes.io/tls-acme: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-url: https://auth.plexmox.com/api/verify
      nginx.ingress.kubernetes.io/auth-signin: https://auth.plexmox.com

    hosts:
      - grafana.plexmox.com

    tls:
      - secretName: grafana-plexmox-com-tls
        hosts:
          - grafana.plexmox.com

  sidecar:
    dashboards:
      folderAnnotation: grafana_folder
      provider:
        foldersFromFilesStructure: true

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: Custom
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      haproxy:
        gnetId: 12693
        revision: 7
        datasource: Prometheus
      node_exporter:
        gnetId: 1860
        revision: 33
        datasource: Prometheus
      argocd:
        gnetId: 14584
        revision: 1
        datasource: Prometheus
      cert-manager:
        gnetId: 11001
        revision: 1
        datasource: Prometheus
      ingress-nginx:
        gnetId: 9614
        revision: 1
        datasource: Prometheus
      mysql-overview:
        gnetId: 14057
        revision: 1
        datasource: Prometheus
      golang:
        gnetId: 14061
        revision: 1
        datasource: Prometheus
      redis:
        gnetId: 14091
        revision: 1
        datasource: Prometheus

  additionalDataSources:
    - name: Loki
      ordId: 1
      access: proxy
      type: loki
      url: http://loki-gateway
      jsonData:
        maxLines: 250

prometheus:
  ingress:
    enabled: true

    annotations:
      kubernetes.io/tls-acme: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      nginx.ingress.kubernetes.io/auth-url: https://auth.plexmox.com/api/verify
      nginx.ingress.kubernetes.io/auth-signin: https://auth.plexmox.com

    hosts:
      - prometheus.plexmox.com

    tls:
      - secretName: prometheus-plexmox-com-tls
        hosts:
          - prometheus.plexmox.com

  persistentVolume:
    enabled: true
    size: 50Gi
    storageClass: longhorn

  prometheusSpec:
    # ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false

    retention: 7d

    additionalScrapeConfigs:
      - job_name: kubernetes-service-endpoints
        kubernetes_sd_configs:
        - role: service
        relabel_configs:

        # annotation 'prometheus.io/scrape' must be set to 'true'
        - action: keep
          regex: true
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]

        # service cannot be in kube-system or prom namespaces
        - action: drop
          regex: (kube-system|prom)
          source_labels: [__meta_kubernetes_namespace]

        # service port name must end with word 'metrics'
        - action: keep
          regex: .*metrics
          source_labels: [__meta_kubernetes_service_port_name]

        # allow override of http scheme
        - action: replace
          regex: (https?)
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          target_label: __scheme__

        # allow override of default /metrics path
        - action: replace
          regex: (.+)
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          target_label: __metrics_path__

        # allow override of default port
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          target_label: __address__

        - {action: labelmap, regex: __meta_kubernetes_service_label_(.+)}

        - action: replace
          source_labels: [__meta_kubernetes_namespace]
          target_label: kubernetes_namespace

        - action: replace
          source_labels: [__meta_kubernetes_service_name]
          target_label: kubernetes_name

      - job_name: 'haproxy'
        static_configs:
          - targets:
            - 'jump.lab:8404'
            - 'k8s.lab:8404'

      - job_name: 'mariadb'
        static_configs:
          - targets:
            - 'mariadb-1.lab:9104'
            - 'mariadb-2.lab:9104'

      - job_name: 'node_exporter'
        static_configs:
          - targets:
            - 'pihole.lab:9100'
            - 'lb-1.lab:9100'
            - 'lb-2.lab:9100'
            - 'vpn-1.lab:9100'
            - 'vpn-2.lab:9100'
            - 'jump.lab:9100'
            - 'proxmox-1.lab:9100'
            - 'proxmox-2.lab:9100'
            - 'k3s-control-1.lab:9100'
            - 'k3s-control-2.lab:9100'
            - 'k3s-control-3.lab:9100'
            - 'k3s-worker-1.lab:9100'
            - 'k3s-worker-2.lab:9100'
            - 'mariadb-1.lab:9100'
            - 'mariadb-2.lab:9100'

      - job_name: 'pve'
        static_configs:
          - targets:
            - proxmox-1.lab
            - proxmox-2.lab
        metrics_path: /pve
        params:
          module: [default]
        relabel_configs:
          - source_labels: [__address__]
            target_label: __param_target
          - source_labels: [__param_target]
            target_label: instance
          - target_label: __address__
            replacement: proxmox-exporter.monitoring.svc:9221

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
